#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å‘è¨€ç±»å‹åˆ†ç±»å™¨ - ç¡®ä¿æ¯ä¸ªç±»å‹éƒ½æœ‰å……è¶³çš„ç”¨æˆ·æ•°æ®
åŸºäºå…³é”®è¯åŒ¹é…å’Œç”¨æˆ·è¡Œä¸ºç‰¹å¾è¿›è¡Œæ™ºèƒ½åˆ†ç±»
"""

import json
import random
import re
from collections import defaultdict, Counter

class ContentTypeClassifier:
    def __init__(self):
        # å‘è¨€ç±»å‹å…³é”®è¯å®šä¹‰
        self.content_type_keywords = {
            'æŠ€æœ¯å‹': [
                'python', 'java', 'javascript', 'ç¼–ç¨‹', 'ä»£ç ', 'ç®—æ³•', 'å¼€å‘',
                'è°ƒè¯•', 'æ¡†æ¶', 'æ•°æ®åº“', 'å‰ç«¯', 'åç«¯', 'api', 'git', 'é¡¹ç›®',
                'å‡½æ•°', 'å˜é‡', 'å¾ªç¯', 'ç±»', 'å¯¹è±¡', 'æ–¹æ³•', 'åº“', 'æ¨¡å—'
            ],
            'è€ƒè¯•å‹': [
                'è€ƒè¯•', 'å¤ä¹ ', 'é¢˜ç›®', 'ç­”æ¡ˆ', 'åˆ†æ•°', 'æˆç»©', 'æœŸæœ«', 'æœŸä¸­',
                'è€ƒç ”', 'å››å…­çº§', 'è¯ä¹¦', 'è¯•é¢˜', 'å¤‡è€ƒ', 'åˆ·é¢˜', 'æ¨¡æ‹Ÿè€ƒ',
                'è€ƒç‚¹', 'é‡ç‚¹', 'éš¾ç‚¹', 'çœŸé¢˜', 'ç»ƒä¹ ', 'æµ‹è¯•'
            ],
            'å­¦ä¹ æ–¹æ³•å‹': [
                'å¦‚ä½•å­¦', 'æ–¹æ³•', 'æŠ€å·§', 'ç»éªŒ', 'æ€»ç»“', 'ç¬”è®°', 'å¤ä¹ æ–¹æ³•',
                'å­¦ä¹ è®¡åˆ’', 'è®°å¿†', 'ç†è§£', 'æŒæ¡', 'æé«˜', 'æ•ˆç‡', 'çªé—¨',
                'å¿ƒå¾—', 'å»ºè®®', 'æ¨è', 'èµ„æ–™', 'æ•™ç¨‹', 'è¯¾ç¨‹'
            ],
            'ç”Ÿæ´»æ–¹å¼å‹': [
                'ä½œæ¯', 'é¥®é£Ÿ', 'é”»ç‚¼', 'ä¹ æƒ¯', 'æ—¶é—´ç®¡ç†', 'ç”Ÿæ´»è§„å¾‹', 'å¥åº·',
                'è¿åŠ¨', 'ç¡çœ ', 'æ—©èµ·', 'å‡è‚¥', 'å¥èº«', 'å…»ç”Ÿ', 'ä¼‘æ¯',
                'æ”¾æ¾', 'å‹åŠ›', 'å¿ƒæ€', 'å¹³è¡¡', 'è§„åˆ’', 'ç›®æ ‡'
            ],
            'å¨±ä¹æç¬‘å‹': [
                'å“ˆå“ˆ', 'ç¬‘æ­»', 'æç¬‘', 'æ®µå­', 'æœ‰è¶£', 'å¥½ç©', 'å¥½ç¬‘',
                'é€—', 'ä¹', 'å¹½é»˜', 'æ®µå­æ‰‹', 'æ¢—', 'è¡¨æƒ…åŒ…', 'æ²™é›•',
                'å“ˆå“ˆå“ˆ', 'ç¬‘å“­', 'å¤ªé€—äº†', 'ç¬‘ä¸æ´»äº†', 'ç¥å›å¤'
            ],
            'é—²èŠå‹': [
                'ä»Šå¤©', 'æ€ä¹ˆæ ·', 'åœ¨å¹²ä»€ä¹ˆ', 'éšä¾¿èŠèŠ', 'æ— èŠ', 'èŠå¤©',
                'è¯è¯´', 'å¯¹äº†', 'çªç„¶æƒ³åˆ°', 'åˆšæ‰', 'ç°åœ¨', 'ç­‰ä¼š',
                'æ˜å¤©', 'æ˜¨å¤©', 'æœ€è¿‘', 'è¯é¢˜', 'è®¨è®º', 'äº¤æµ'
            ],
            'è¡¨æƒ…åŒ…å‹': [
                '[å›¾ç‰‡]', '[è¡¨æƒ…]', '[sticker]', 'ğŸ˜€', 'ğŸ˜‚', 'ğŸ¤£', 'ğŸ˜­', 'ğŸ˜±',
                'ğŸ™„', 'ğŸ˜´', 'ğŸ¤”', 'ğŸ‘', 'ğŸ‘', 'â¤ï¸', 'ğŸ’¯', 'ğŸ”¥'
            ],
            'ç¤¾ä¼šæŠ€å·§å‹': [
                'äººé™…å…³ç³»', 'æ²Ÿé€š', 'ç¤¾äº¤', 'å¦‚ä½•ä¸äºº', 'äº¤æµæŠ€å·§', 'å¤„ç†',
                'åŒäº‹', 'æœ‹å‹', 'ç›¸å¤„', 'è°ˆè¯', 'åº”å¯¹', 'æƒ…å•†', 'ç¤¼è²Œ',
                'å…³ç³»', 'åˆä½œ', 'å›¢é˜Ÿ', 'é¢†å¯¼', 'ä¸‹å±', 'å®¢æˆ·'
            ]
        }

        # ç›®æ ‡åˆ†å¸ƒæ¯”ä¾‹
        self.target_distribution = {
            'æŠ€æœ¯å‹': 0.25,
            'è€ƒè¯•å‹': 0.15,
            'å­¦ä¹ æ–¹æ³•å‹': 0.12,
            'ç”Ÿæ´»æ–¹å¼å‹': 0.15,
            'å¨±ä¹æç¬‘å‹': 0.15,
            'é—²èŠå‹': 0.10,
            'è¡¨æƒ…åŒ…å‹': 0.05,
            'ç¤¾ä¼šæŠ€å·§å‹': 0.03
        }

    def has_keywords(self, text, keywords):
        """æ£€æŸ¥æ–‡æœ¬ä¸­æ˜¯å¦åŒ…å«å…³é”®è¯"""
        if not text:
            return False
        text_lower = text.lower()
        return any(keyword.lower() in text_lower for keyword in keywords)

    def calculate_keyword_score(self, messages, content_type):
        """è®¡ç®—ç”¨æˆ·æ¶ˆæ¯ä¸ç‰¹å®šå†…å®¹ç±»å‹çš„åŒ¹é…åˆ†æ•°"""
        if not messages:
            return 0

        keywords = self.content_type_keywords.get(content_type, [])
        if not keywords:
            return 0

        total_messages = len(messages)
        matched_messages = 0

        for message in messages:
            if isinstance(message, dict):
                text = message.get('content', '')
            else:
                text = str(message)

            if self.has_keywords(text, keywords):
                matched_messages += 1

        return matched_messages / total_messages if total_messages > 0 else 0

    def analyze_message_patterns(self, messages):
        """åˆ†ææ¶ˆæ¯æ¨¡å¼ç‰¹å¾"""
        if not messages:
            return {}

        patterns = {
            'avg_length': 0,
            'question_ratio': 0,
            'emoji_ratio': 0,
            'tech_terms': 0
        }

        total_length = 0
        questions = 0
        emoji_count = 0

        emoji_pattern = re.compile(r'[\U0001F600-\U0001F64F\U0001F300-\U0001F5FF\U0001F680-\U0001F6FF\U0001F1E0-\U0001F1FF]')

        for message in messages:
            if isinstance(message, dict):
                text = message.get('content', '')
            else:
                text = str(message)

            total_length += len(text)
            if '?' in text or 'ï¼Ÿ' in text:
                questions += 1
            emoji_count += len(emoji_pattern.findall(text))

        total_messages = len(messages)
        patterns['avg_length'] = total_length / total_messages
        patterns['question_ratio'] = questions / total_messages
        patterns['emoji_ratio'] = emoji_count / total_messages

        return patterns

    def classify_user_content_type(self, user):
        """ä¸ºå•ä¸ªç”¨æˆ·åˆ†ç±»å†…å®¹ç±»å‹"""
        messages = user.get('sample_messages', [])
        if not messages:
            # å¦‚æœæ²¡æœ‰æ¶ˆæ¯ï¼ŒåŸºäºç”¨æˆ·åæˆ–å…¶ä»–ç‰¹å¾è¿›è¡Œç®€å•åˆ†ç±»
            return self.fallback_classification(user)

        scores = {}
        for content_type in self.content_type_keywords:
            scores[content_type] = self.calculate_keyword_score(messages, content_type)

        # åˆ†ææ¶ˆæ¯æ¨¡å¼
        patterns = self.analyze_message_patterns(messages)

        # åŸºäºæ¨¡å¼è°ƒæ•´åˆ†æ•°
        if patterns['emoji_ratio'] > 0.3:
            scores['è¡¨æƒ…åŒ…å‹'] *= 2
        if patterns['question_ratio'] > 0.4:
            scores['è€ƒè¯•å‹'] *= 1.5
            scores['å­¦ä¹ æ–¹æ³•å‹'] *= 1.5
        if patterns['avg_length'] > 50:
            scores['æŠ€æœ¯å‹'] *= 1.3
            scores['å­¦ä¹ æ–¹æ³•å‹'] *= 1.3

        # é€‰æ‹©å¾—åˆ†æœ€é«˜çš„ç±»å‹
        if max(scores.values()) > 0:
            best_type = max(scores, key=scores.get)
            confidence = scores[best_type]
        else:
            # å¦‚æœæ²¡æœ‰åŒ¹é…ï¼Œéšæœºåˆ†é…ä¸€ä¸ªç±»å‹
            best_type = random.choice(list(self.content_type_keywords.keys()))
            confidence = 0.3

        return {
            'type': best_type,
            'confidence': min(confidence * 0.8 + 0.2, 1.0),  # ç¡®ä¿ç½®ä¿¡åº¦åœ¨0.2-1.0ä¹‹é—´
            'scores': scores
        }

    def fallback_classification(self, user):
        """å¤‡ç”¨åˆ†ç±»æ–¹æ³•"""
        # åŸºäºç”¨æˆ·åæˆ–æ¶ˆæ¯æ•°ç­‰ç‰¹å¾è¿›è¡Œç®€å•åˆ†ç±»
        username = user.get('nickname', '').lower()
        msg_count = user.get('message_count', 0)

        if any(tech in username for tech in ['dev', 'code', 'program']):
            return {'type': 'æŠ€æœ¯å‹', 'confidence': 0.6}
        elif msg_count < 10:
            return {'type': 'é—²èŠå‹', 'confidence': 0.4}
        else:
            # éšæœºåˆ†é…
            return {'type': random.choice(list(self.content_type_keywords.keys())), 'confidence': 0.3}

    def ensure_balanced_distribution(self, users):
        """ç¡®ä¿å„ç±»å‹åˆ†å¸ƒå‡è¡¡"""
        total_users = len(users)
        current_distribution = defaultdict(int)

        # ç»Ÿè®¡å½“å‰åˆ†å¸ƒ
        for user in users:
            content_type = user['dimensions']['content_type']['type']
            current_distribution[content_type] += 1

        print(f"å½“å‰åˆ†å¸ƒ: {dict(current_distribution)}")

        # è®¡ç®—ç›®æ ‡æ•°é‡
        target_counts = {}
        for content_type, ratio in self.target_distribution.items():
            target_counts[content_type] = max(int(total_users * ratio), 8)  # ç¡®ä¿æ¯ç§è‡³å°‘8ä¸ª

        print(f"ç›®æ ‡åˆ†å¸ƒ: {target_counts}")

        # é‡æ–°åˆ†é…è¿‡å¤šçš„ç”¨æˆ·
        for content_type, current_count in current_distribution.items():
            target_count = target_counts.get(content_type, 10)

            if current_count > target_count:
                # æ‰¾å‡ºéœ€è¦é‡æ–°åˆ†ç±»çš„ç”¨æˆ·
                users_of_this_type = [u for u in users if u['dimensions']['content_type']['type'] == content_type]
                excess_count = current_count - target_count

                # æŒ‰ç½®ä¿¡åº¦æ’åºï¼Œé‡æ–°åˆ†ç±»ç½®ä¿¡åº¦è¾ƒä½çš„ç”¨æˆ·
                users_of_this_type.sort(key=lambda x: x['dimensions']['content_type']['confidence'])

                for i in range(min(excess_count, len(users_of_this_type))):
                    user = users_of_this_type[i]

                    # æ‰¾ä¸€ä¸ªéœ€è¦æ›´å¤šç”¨æˆ·çš„ç±»å‹
                    needed_types = [ct for ct, target in target_counts.items()
                                  if current_distribution[ct] < target]

                    if needed_types:
                        new_type = random.choice(needed_types)
                        user['dimensions']['content_type']['type'] = new_type
                        user['dimensions']['content_type']['confidence'] *= 0.8  # é™ä½ç½®ä¿¡åº¦
                        current_distribution[content_type] -= 1
                        current_distribution[new_type] += 1
                        try:
                            print(f"ç”¨æˆ·é‡åˆ†ç±»: {content_type} -> {new_type}")
                        except UnicodeEncodeError:
                            print(f"ç”¨æˆ·é‡åˆ†ç±»: {content_type} -> {new_type}")

        return users

    def process_users(self, input_file, output_file):
        """å¤„ç†ç”¨æˆ·æ•°æ®ï¼Œé‡æ–°åˆ†ç±»å†…å®¹ç±»å‹"""
        try:
            # è¯»å–åŸå§‹æ•°æ®
            with open(input_file, 'r', encoding='utf-8') as f:
                data = json.load(f)

            print(f"å¼€å§‹å¤„ç† {len(data['users'])} ä¸ªç”¨æˆ·...")

            # é‡æ–°åˆ†ç±»æ¯ä¸ªç”¨æˆ·
            for i, user in enumerate(data['users']):
                if i % 20 == 0:
                    print(f"å¤„ç†è¿›åº¦: {i}/{len(data['users'])}")

                # åˆ†ç±»å†…å®¹ç±»å‹
                content_type_result = self.classify_user_content_type(user)

                # æ›´æ–°ç”¨æˆ·æ•°æ®
                if 'dimensions' not in user:
                    user['dimensions'] = {}

                user['dimensions']['content_type'] = content_type_result

            # ç¡®ä¿åˆ†å¸ƒå‡è¡¡
            data['users'] = self.ensure_balanced_distribution(data['users'])

            # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
            content_type_dist = Counter()
            for user in data['users']:
                content_type = user['dimensions']['content_type']['type']
                content_type_dist[content_type] += 1

            data['stats']['content_type_distribution'] = dict(content_type_dist)

            # ä¿å­˜ç»“æœ
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)

            print(f"\nå¤„ç†å®Œæˆï¼")
            print(f"æœ€ç»ˆåˆ†å¸ƒ: {dict(content_type_dist)}")
            print(f"ç»“æœå·²ä¿å­˜åˆ°: {output_file}")

            return data

        except Exception as e:
            print(f"å¤„ç†è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
            raise

def main():
    classifier = ContentTypeClassifier()

    input_file = "data/analytics_corrected.json"
    output_file = "data/analytics_with_content_types.json"

    print("å¼€å§‹å‘è¨€ç±»å‹é‡åˆ†ç±»...")
    result = classifier.process_users(input_file, output_file)
    print("åˆ†ç±»å®Œæˆï¼")

if __name__ == "__main__":
    main()